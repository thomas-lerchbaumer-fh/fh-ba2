%----------------------------------------------------------------
%
%  File    :  prediction_model.tex
%
%  Authors : Thomas Lerchbaumer
% 
%  Created :  19 March 2022
% 
%  Changed :  19 March
% 
%----------------------------------------------------------------
\chapter{Implementation of LSTM and CNN}
\label{chap:predict}
\section{Implementation}
\label{sec:implementation}
Both models follow the same workflow when it comes to their implementation. The workflow is highlighted in figure \ref{fig:workflow}:

\begin{figure}[H]
	\centering
		\includegraphics[width=12cm]{images/workflow_imp}
	\caption{Workflow of the implementation - [source:[author]]}
	\label{fig:workflow}
\end{figure}

As each booking correlates to one entry within the database table those entries need to be grouped on a daily basis. Therefore the following sql query is executed: 
\begin{lstlisting}
import mysql.connector
import logging
import pandas as pd

def get_booking_data():

       sql = ("SELECT count(taskFrom_time) as bookings, date(taskFrom_time) "
           "from bookings_2 "
           "WHERE date(taskFrom_time) <= DATE('2023-05-01') and date(taskFrom_time) >= 			DATE('2017-01-01') "
           "Group By date(taskFrom_time) order by date(taskFrom_time) asc")

    res = pd.read_sql(sql, connection)
    return res
\end{lstlisting}

Once the data is retrieved from the database it is directly converted to a \verb|pandas dataframe|. As mentioned in chapter \ref{sec:data_aug} data augmentation is necessary to compensate the lack of data during Covid19. Corresponding to the workflow described in figure \ref{fig:workflow} the data now needs to be separated into a training and test set. The test set is used to assess how well the trained model performs, whereas the training set is used to train the model. The trained model performs predictions for timestamps used in the test set. By reviewing other scientific works that deal with time series forecasting \cite{1d_cnn}\cite{cnn_vechicle}\cite{cnn_intro}\cite{lstm_overcome_rnn_problem}\cite{lstm_module}\cite{lstm_stock} the most accurate results are achieved by using ranges from 80 \% to 90 \% for training and depending on the range for training data a range of 20 \% to 10 \% for test data are recommended. The initial split used for both models correlates to 90\% training data to 10 \% test data as visualised in figure \ref{fig:training_test}. Therefore the following code is applied to split the data:
\begin{lstlisting}
training_end = pd.to_datetime('2022-10-31')
#total range 2017-01-01 - 2023-05-31 = 2342 days
train = df[:training_end]
# 2017-01-01 - 2022-10-31 = 2129 days ~ 90%
test = df[training_end:]
# 2022-10-31 - 2023-05-01 = 213 days ~ 10%
\end{lstlisting}
\begin{figure}[H]
	\centering
		\includegraphics[width=14cm]{images/1st_model_training_split}
	\caption{Visualised Training-Test split (90\%-10\%) - [source:[author]]}
	\label{fig:training_test}
\end{figure}
With the training and test data in place the next step is to implement both models (LSTM, CNN) by utilising the \verb|tensorflow| library. To reduce computation costs and to increase training efficiency the training data is further processed in the following way:
\begin{lstlisting}
WINDOW = 20
bookings_training_90 = tf.data.Dataset.from_tensor_slices(train.values)
bookings_training_90 = bookings_training_90.window(WINDOW + 1, shift=1, drop_remainder=True)
bookings_training_90 = bookings_training_90.flat_map(lambda x: x.batch(WINDOW + 1))
bookings_training_90 = bookings_training_90.map(lambda x: (x[:-1], x[-1]))
\end{lstlisting}
On line 2 the code above transforms the training data into a \verb|TensorSliceDataset|. This data format grants access to tensorflow's data API which supports the user to manipulate the data further. As this is time series data the model itself is fed with data limited to certain ranges. The constant \verb|WINDOW| indicates how many consecutive timestamps are used as input to predict the next time step. The function \verb|flat.map()| is now used to flatten the dataset. As \verb|from_tensor_slices()| creates a single tensor for each entry in a window \verb|flat.map()| combines those windows to a single tensor holding the windowed data. Line 5 prepares the data and splits each window into features and target values. As the learning process described in section \ref{sec:bp} involves multiple inputs that are used to predict one output the \verb|.map()| function prepares each windowed tensor by using the interval from \verb|x to x-1| to predict \verb|x-1|.
Both models LSTM and CNN are trained with the prepared data explained in during this section. 

\subsection{Implementation of LSTM}
The code itself required to setup a LSTM model only requires a few input parameters. Therefore it is crucial to understand the meaning behind the input parameters as well as how they can influence the training results of the model itself. The following code is used to initialise the model:   
\begin{lstlisting}
#define the model
lstm_booking_prediction_model = Sequential([
    Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[WINDOW]),
    Bidirectional(LSTM(128, activation='tanh',recurrent_activation='sigmoid' )),
    Dense(units=128, activation='relu'),
    Dropout(0.4),
    Dense(1)
	])
\end{lstlisting}
When utilising LSTM for time series predictions a certain input format for data is needed. Therefore the Lambda function on line 3 is required to reshape the dimension of the used input data. 
\verb|Bidirectional()|\footnote{\label{tf_fn}\url{https://www.tensorflow.org/api_docs/python/tf/keras/}} actually represents a wrapper for the actual layer used for this model. In this case it is holding additional states and is used to create a Bi-LSTM model as described in section \ref{sec:lstm}. \newline
\verb|LSTM()|\textsuperscript{\ref{tf_fn}} contains actual logic as described in section \ref{sec:lstm}. Moreover the parameter \verb|unit=128|\textsuperscript{\ref{tf_fn}} can be understood as the number of neurons used within this layer. Furthermore this model offers different kinds of activation functions. By default the hyperbolic tangent (tanh)\cite{tanh} is used. Whereas the \verb|recurrent_activation| defines which functions are used for the actual gates within the module as described in section \ref{sec:lstm}. Whenever creating a stacked LSTM model, which means it makes use of at least two LSTM layers the parameter \verb|return_sequences| must be set to true. Otherwise the layer's output results in a 2d tensor output which only provides information about the last timestep. This format cannot be passed on to the next LSTM layer. \newline
\verb|Dense()| is used to implement fully connected layers whereas \verb|units| correlate to the amount of neurons used for this layer. The last \verb|dense(1)| layer is used to reduce the number of outputs to 1. The way this layer works is explained in section \ref{sec:cnn}. Additionally the model needs to be compiled. Therefore the following code is required: 
\begin{lstlisting}
#compile the model 
lstm_booking_prediction_model.compile(
    loss=Huber(),
    optimizer=Adam(),
    metrics=['mae']
	)
\end{lstlisting}
The parameter \verb|loss| sets the loss function which is utilised to evaluate the models performance. The reason why \verb|Huber| is used is explained in section \ref{sec:loss_func}. Futhermore the model also requires an optimising algorithm. This algorithm is assigned by using the \verb|optimizer| parameter. Due to the results the literature review explained in section \ref{sec:optimize_func} Adam turned out to be the most promising candidate for this purpose. To observe the capability of learning of a given model the metric mean absolute error (MEA) can be used. 

The next step is to start the training of the model as demonstrated below:
\begin{lstlisting}
#start to train the model
lstm_history = lstm_booking_prediction_model.fit(
    bookings_training_90,
    epochs=100,
    verbose=1,
    use_multiprocessing=True
	)
\end{lstlisting}
The function \verb|fit()| requires parameters like the data used for training (\verb|bookings_training_90|) as well as how many epochs are processed. One \verb|epoch| covers the process of iterating through the entire training dataset and performing forward and backward propagation as well as updating the model's parameters based on the chosen optimisation algorithm.
\newline
When training the model using the initial parameters described during this chapter the following results are achieved:
\begin{figure}[H]
	\centering
		\includegraphics[width=14cm]{images/lstm_1_paper_training}
	\caption{Decrease of MAE and Trainig loss for 100 epochs - [source:[author]]}
	\label{fig:training_test}
\end{figure}
In general, the training loss represents how well the model performed using the training data. It is generated through the output of the applied loss function. Low values basically mean that the model is able to catch up patterns within the training set but could also indicate overfitting. This causes the model to just memorize the training set rather than learning actual patterns. Furthermore the loss indicates the amount of required epochs for the model to learn. Whenever the loss stalls or is starting to increase again the training should be stopped. The MAE is another metric to observe the ability of the model to learn. The MAE is obtained by calculating the mean of actual values minus the predicted values. This result is divided by the number of observations.\cite{mae} Similar to the training loss a lower value indicates the performance of the model but the MAE needs to be interpreted in context with the range the predictions are made for. For example a MAE of 1 for predictions made in ranges between 1-2 is interpreted as poor performance, whereas a MAE of 100 for predictions made in a range between 100.000 and 1.000.000 indicates a proper result.

\begin{figure}[H]
	\centering
		\includegraphics[width=14cm]{images/paper_1_lstm_1_prediction}
	\caption{Predictions on test set from 01-01-203 to 01-05-2023 - [source:[author]]}
	\label{fig:lstm_1_training_test}
\end{figure}
By looking at figure \ref{fig:lstm_1_training_test} it is clear to see that the model's performance in terms of accuracy is insufficient. Investigating the training loss and MAE shows that the final MAE of around 0.5 is acceptable in context of the prediction range ranging from 1 to 17. Given that the final training loss is also minimal, this suggests that the model has a tendency to overfit. Section \ref{sec:improving} investigates strategies on how to improve the model's performance.

\subsection{Implementation of CNN}
Similar to the implementation of LSTM the implementation of a CNN does not require much code. Furthermore some parts of the code use the same parameters. Those parameters are not explained again. To initialise a CNN model the following code is required: 

 \begin{lstlisting}
#define the model 
    Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[WINDOW]),
    Conv1D(filters=128, kernel_size=10, strides=1,
           padding='causal', activation='relu'),
    Conv1D(filters=128, kernel_size=10, strides=1,
           padding='causal', activation='relu'),
    GlobalAveragePooling1D(),
    Flatten(),
    Dropout(0.4),
    Dense(1)
\end{lstlisting}
\verb|Conv1D()| is used to add one 1d CL to the model. Those layers work as described in section \ref{sec:cnn}.  The \verb|filter| parameter is equivalent to the number of neurons this layer is going to use. Furthermore the kernel's size as explained in section \ref{sec:cnn} is determined by the parameter \verb|kernel_size|.
The only difference left in comparison to the LSTM layer is the FL describe in section \ref{sec:cnn}. This layer is added by utilising the \verb|Flatten()| parameter. When it comes to compiling and fitting the model the CNN makes use of  \verb|Huber()| for its loss function and \verb|Adam()| for its optimisation function contrary to the implementation of the LSTM model. Furthermore both models are initially using 100 epochs for their first training. \newline

The first training results produced by the CNN model look the following: 

\begin{figure}[H]
	\centering
		\includegraphics[width=14cm]{images/cnn_model_1_loss}
	\caption{Decrease of MAE and Trainig Loss for 100 epochs - [source:[author]]}
	\label{fig:training_test_cnn}
\end{figure}
When analysing the metrics highlighted in figure \ref{fig:training_test_cnn} it is clear to see that both the training loss and the MAE are still very high after 100 epochs. This suggests that more epochs should be used to train the model. The results of the training loss as well as the MAE are reflected also in the actual predictions made on the test set.

\begin{figure}[H]
	\centering
		\includegraphics[width=14cm]{images/cnn_1_prediction}
	\caption{Predictions on test set from 01-01-203 to 01-05-2023 - [source:[author]]}
	\label{fig:cnn_1_training_test}
\end{figure}
By looking at figure \ref{fig:cnn_1_training_test} it is clear to see that the model's performance is poor. During it is training the model was not capable of obtaining the training's set characteristics. Strategies on how to potentially improve the models accuracy are discussed in section \ref{sec:improving}.

\section{Improving the models}
\label{sec:improving}
There are several strategies that can be followed in order to improve a model's performance. One option is to enhance the data preprocessing. Therefore the available data set is normalised. One approach that can be utilised is Min-Max Normalisation \cite{min_max}. Following this approach the original data is brought into a range from values between 0 and 1 by keeping the actual ratio of the test and training data. 
Another approach is called feature engineering \cite{feature_eng}. Following this strategy further features are extracted from the available dataset. This process results in a multivariate prediction model \cite{multi}.
Additional possibilities to enhance a model's performance is to change its architecture. This implies methods like utilising multiple \verb|Conv1D| layers or stacking several LSTM layers. Applying this approach leads to more complex models which might improve a NN's ability to find patterns within the available dataset. Similar to this approach the tuning of a model's parameters is also called hyperparameter tuning.  This method applies different configurations of parameters to train the model and compares their results. Looking at the implementation of the LSTM  and CNN model described in section \ref{sec:implementation} parameters like \verb|epochs|, \verb|window|, \verb|units|(LSTM) or \verb|filters|(CNN), and \verb|Dropout|. As there is no model that fits all purposes those parameters are updated during an iterative process. \newline
One of the approaches to increase the model's performance in this section is a combination of changing the architecture of both models as well as changing their hyper parameters. 

\subsection{Adapting the Architecture and Hyperparameter tuning}
\label{sec:hyper}
This section proposes various configurations that are applied to both models in order to increase their accuracy. Therefore table \ref{tab:models} lists the various combinations used to train the models.

\newsavebox{\codebox} % For saving code in the first row
\newsavebox{\codeboxtwo} % For saving code in the second row
\newsavebox{\codeboxthree} % lstm 3
\newsavebox{\codeboxfour} % cnn 1
\newsavebox{\codeboxfive} % cnn 2

\newsavebox{\codeboxsix} % cnn 3

\begin{lrbox}{\codeboxsix} % Start saving into \codebox lstm1
\begin{lstlisting}[numbers=none, basicstyle=\tiny, numbersep=0pt, xleftmargin=0pt, xrightmargin=0pt, backgroundcolor=\color{white}]
Conv1D(filters=256, kernel_size=3, strides=1, 
	   padding='causal', activation='relu')
BatchNormalization()
Conv1D(filters=256, kernel_size=3, strides=1,
	   padding='causal',activation='relu')
BatchNormalization()
Conv1D(filters=256, kernel_size=3, strides=1, 
	   padding='causal', activation='relu')
BatchNormalization()
Flatten()
Dropout(0.4)
Dense(1)
\end{lstlisting}
\end{lrbox} % Stop saving into \codebox



\begin{lrbox}{\codeboxfive} % Start saving into \codebox lstm1
\begin{lstlisting}[numbers=none, basicstyle=\tiny, numbersep=0pt, xleftmargin=0pt, xrightmargin=0pt, backgroundcolor=\color{white}]
Conv1D(filters=64, kernel_size=10, strides=1,
    padding='causal', activation='relu'),
Dropout(0.2),
Conv1D(filters=64, kernel_size=10, strides=1,
    padding='causal', activation='relu'),
Dropout(0.2),
Conv1D(filters=32, kernel_size=10, strides=1,
    padding='causal', activation='relu'),
GlobalAveragePooling1D(),
Flatten(),
Dropout(0.1),
Dense(1)
\end{lstlisting}
\end{lrbox} % Stop saving into \codebox


\begin{lrbox}{\codeboxfour} % Start saving into \codebox lstm1
\begin{lstlisting}[numbers=none, basicstyle=\tiny, numbersep=0pt, xleftmargin=0pt, xrightmargin=0pt, backgroundcolor=\color{white}]
Conv1D(filters=256, kernel_size=3, strides=1,
       padding='causal', activation='relu')
Conv1D(filters=256, kernel_size=3, strides=1,
       padding='causal', activation='relu')
GlobalAveragePooling1D()
Flatten()
Dropout(0.4)
Dense(1)
\end{lstlisting}
\end{lrbox} % Stop saving into \codebox

\begin{lrbox}{\codebox} % Start saving into \codebox lstm1
\begin{lstlisting}[numbers=none, basicstyle=\tiny, numbersep=0pt, xleftmargin=0pt, xrightmargin=0pt, backgroundcolor=\color{white}]
Bidirectional(LSTM(units = 80,
 	return_sequences=False,activation="relu")),
Dropout(0.2),
Dense(1)
\end{lstlisting}
\end{lrbox} % Stop saving into \codebox


\begin{lrbox}{\codeboxtwo} % Start saving into \codeboxtwo lstm2
\begin{lstlisting}[numbers=none, basicstyle=\tiny, numbersep=0pt, xleftmargin=0pt, xrightmargin=0pt, backgroundcolor=\color{white}]
Bidirectional(LSTM(units = 150,
	return_sequences=True,activation="relu")),
Bidirectional(LSTM(units = 150,
	return_sequences=False,activation="relu")),
Dropout(0.2),
Dense(1)
\end{lstlisting}
\end{lrbox} % Stop saving into \codeboxtwo

\begin{lrbox}{\codeboxthree} % Start saving into \codeboxtwo lstm2
\begin{lstlisting}[numbers=none, basicstyle=\tiny, numbersep=0pt, xleftmargin=0pt, xrightmargin=0pt, backgroundcolor=\color{white}]
Bidirectional(LSTM(units = 120, 
	return_sequences=True,activation="relu")),
Dropout(0.3),
Bidirectional(LSTM(units = 120, 
	return_sequences=False,activation="relu")),
Dropout(0.2),
Dense(1))
\end{lstlisting}
\end{lrbox} % Stop saving into \codeboxtwo

\begin{table}[H]
\begin{tabular}{|l|p{9cm}|r|r|r|}
  \hline
  \textbf{Type} & \textbf{Model} & \textbf{Epochs} & \textbf{Loss} & \textbf{MAE} \\
  \hline
  LSTM 1& \usebox{\codebox} & 150 & 0.74 & 1.11 \\
  \hline
  LSTM 2& \usebox{\codeboxtwo} & 250 & 0.25 & 0.55 \\
  \hline
  LSTM 3& \usebox{\codeboxtwo} & 300 & 0.20 & 0.35 \\
  \hline
    CNN 1 & \usebox{\codeboxfour} & 250 & 0.81 & 1.19 \\
  \hline
   CNN 2& \usebox{\codeboxfive} & 350 & 0.25 & 0.51 \\
  \hline
   CNN 3& \usebox{\codeboxsix} & 500 & 0.21 & 0.43 \\
  \hline
\end{tabular}
\caption{Various model combinations including epochs, final loss and final MAE - [source:[author]]}
	\label{tab:models}
\end{table}

By observing the results from table \ref{tab:models} it is clear to see that by increasing the epochs for approaches using CNN the actual loss as well as the MAE decreased. Nevertheless as illustrated in figure \ref{fig:combined_model} both models (CNN 3 and LSTM 3) still were not able to catch up the characteristics of the training set. Whereas some predictions are on point the majority of predictions are shifted. Neither increasing nor decreasing the models' complexities in terms of architecture; supported the models to further improve their accuracy for predictions based on the test set.
This behaviour indicates that only one feature might be insufficient for the models to find patterns that increase their ability to predict demand for buses on a certain day.
Therefore section \ref{sec:feature_eng} investigates which potential features can be extracted from the current dataset as well as their impact on prediction accuracy. Although none of the trained models achieve adequate results models that used LSTM achieved better results throughout the whole process. Therefore only LSTM models are utilised during section \ref{sec:feature_eng}.
\begin{figure}[H]
	\centering
		\includegraphics[width=14cm]{images/combined_prediction}
	\caption{Predictions of CNN 3 and LSTM 3 - [source:author]}
	\label{fig:combined_model}
\end{figure}

\subsection{Feature Engineering}
\label{sec:feature_eng}
Feature Engineering (FE) is used to create new input values also called features. Those values are further used to enhance a model's capability of learning. \cite{feature_eng_2} The approaches before used the whole timestamp \verb|tasTime_from| resulting that only one feature is available to train the models.  
Looking at the structure of a date like 2023-01-01 additional features become visible. Those values can be extracted and used to train the model.  The structure essentially hosts aspects like \verb|day of the month|, \verb|month|,\verb|year|, \verb|quarter of the year| as well as \verb|week of the year|. Furthermore this approach provides the possibility to provide additional context to the model like holidays. The insights gathered during chapter \ref{chap:insights} furthermore revealed  seasonal patterns. By extracting those characteristics form every single date more features become available to train the model. As the LSTM model in general looks like the more suitable candidate due to the past results it provided during section \ref{sec:implementation} and section \ref{sec:hyper} the approach of feature engineering is only applied onto the LSTM approach.
\newline
In order to extract additional features the following code is applied: 
\begin{lstlisting}
df['month'] = df.index.month
df['day'] = df.index.day
df['dayofweek'] = df.index.dayofweek
df['quarter'] = df.index.quarter
df['year'] = df.index.year
df["weekend"] = np.where(df.index.day_name().isin(['Saturday', 'Sunday']), 1, 0)
df['week_of_year'] = df.index.isocalendar().week
austrian_holidays = Austria(years=list(range(2017, 2024)))
df['holiday'] = df.index.map(lambda x: int(x in austrian_holidays))
df["main_season"] = np.where(df["month"].isin([5, 6, 7, 8, 9]), 1, 0)
df["off_season"] = np.where(df["month"].isin([5, 6, 7, 8, 9]), 0, 1)
df["ut"] = (df["bookings"] > high_ut).astype(float)
\end{lstlisting}
The initial dataframe remains untouched and got the same structure as in section \ref{sec:implementation}. As the index of the \verb|df| is initialised as \verb|pd.to_datetime()| operations like \verb|index.month|, \verb|index.day| etc. become available. Those functions extract the values based on each index available within the dataframe. 
For the initial training of the multivariate LSTM model the following architecture and parameters are used:
\begin{lstlisting}
Bidirectional(LSTM(120,return_sequences=True,
              input_shape=(X_train.shape[1],X_train.shape[2]))),
Bidirectional(LSTM(120,
              input_shape=(X_train.shape[1],X_train.shape[2]))),
Dropout(0.2),
Dense(1)

lstm_model.compile(
    loss=Huber(),
    optimizer=Adam(),
    metrics=['mae']
)

lstm_model.fit(
X_train, 
Y_train, 
epochs=200, 
)
\end{lstlisting}
As this model now utilises multiple features the parameter \verb|input_shape| needs to be set.  \verb|X_train.shape[1]| represents the number of timesteps whereas \verb|X_train.shape[2]| represents the number of features utilised for each timestep. Furthermore \verb|lstm_model.fit| receives now two input values whereas \verb|X_train| indicates the input features and \verb|Y_train| represents the actual output of a certain time step. For previous models only one input was required due to the way the data is structured. 
The training results of the model are highlighted in figure \ref{fig:lstm_multi_1_mae} and \ref{fig:lstm_multi_1_prediction} 
\begin{figure}[H]
	\centering
		\includegraphics[width=12cm]{images/lstm_multi_1_mae}
	\caption{Change of Loss and MAE - [source:[author]]}
	\label{fig:lstm_multi_1_mae}
\end{figure}
Both the final loss (0.25) and MAE (0.30) are almost identical to the results achieved for the previous models. The major difference is the actual prediction results on the test data highlighted in figure \ref{fig:lstm_multi_1_prediction}. It is clear to see that the model's performance increased dramatically. This behaviour indicates that the model was overfitting the data and not being able to create accurate predictions on the test data. By increasing the context to the LSTM model utilising additional features the models performance on unseen data increased as well. 
\begin{figure}[H]
	\centering
		\includegraphics[width=14cm]{images/lstm_multi_1_prediction}
	\caption{Predictions on test set from 2023-01-01 to 01-05-2023 - [source:[author]]}
	\label{fig:lstm_multi_1_prediction}
\end{figure}
Looking at the results in figure \ref{fig:lstm_multi_1_prediction} the enhanced LSTM model is capable of catching up the trends present in the test data. Section \ref{sec:pred_and_ym} discusses the achieved results and whether or not the model can be used to support the current YM in place.

\section{Predictions and Yield Management}
\label{sec:pred_and_ym}
To determine whether or not the model can be utilised to support the YM knowledge about how far in advance buses are booked is crucial. By computing the median timespan utilising the attributes \verb|createdAt| and \verb|tasTime_from| this information can be gathered from the available dataset. The median timespan between \verb|createdAt| and \verb|taskFrom_time| is around 38 days. Throughout the whole implementation process which included the training of various model architectures as well as singelvariate and multivariate models, models utilising LSTM achieved the best results. 

By looking at the final result achieved in figure \ref{fig:lstm_multi_1_prediction} it is clear to see that the model has the ability to predict future data. Although the results are not perfect for each day the model is able to recognise trends present in the test data. Whereas prediction might be off sometimes the achieved results still can be utilised to support the YM. As YM can have a tremendous influence on customers' decisions to whether or not they book a bus since it changes the pricing security mechanisms need to be set in place. On the one hand predictions are personally checked if they are feasible or not. Furthermore the predictions are set in context with previous years. This context involves seasonal trends and takes holidays in Austria, Germany, Switzerland and Liechtenstein into account. Especially predictions for Easter holidays are monitored since they might occur during different seasons. Additionally the model's performance is continuously monitored and a training schedule is set in place.
As the median for bookings in advance is around 38 days the results achieved for on the test data determine that this range can be predicted accurately.